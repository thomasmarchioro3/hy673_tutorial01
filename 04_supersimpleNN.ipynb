{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Feed-Forward Neural Network in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the pickled data, so we do not have to repeat the preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickledatapath = os.path.join('data', 'iris_data.pkl')\n",
    "\n",
    "with open(pickledatapath, 'rb') as f:\n",
    "    x_train, x_test, y_train, y_test = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that all the data have been correctly imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, torch initializes tensors as 32-bits floating point values.\n",
    "\n",
    "However, it is good practice to set the default type to 64-bits floating point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set the PRNG seeds for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And copypaste our dataset from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyIrisDataset(Dataset): \n",
    "\n",
    "    def __init__(self, x_data, y_data):\n",
    "        super().__init__()\n",
    "        assert len(x_data) == len(y_data)\n",
    "        self.x_data = torch.tensor(x_data, dtype=torch.float64)\n",
    "        self.y_data = torch.tensor(y_data, dtype=torch.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks in Pytorch\n",
    "\n",
    "Finally, it's time to code an actual neural network in Pytorch!\n",
    "\n",
    "A neural network model in Pytorch is an extension of the class `torch.nn.Module`. In order to define this class, we need to specify 2 methods:\n",
    "\n",
    "- `__init__`: In this method, you should declare class variables like the number of features and classes. And, most importantly, you should declare the building blocks of your model. Typically, these will be one or more `Sequential` blocks.\n",
    "- `forward`: Here you declare what your model does during the forward pass. This gives you a lot of flexibility in terms of what your model can do, but also leaves plenty of room for errors!\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMultiLayerPerceptron(nn.Module):\n",
    "\n",
    "    def __init__(self, n_features, n_classes):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(self.n_features, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, self.n_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.model(x)\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To initialize your first model, you just need to create an instance of the model's class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyMultiLayerPerceptron(n_features=4, n_classes=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can print a basic summary of your model's architecture simply by using `print(model)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, this simple print shows the layers of your model, but not the number of trainable parameters.\n",
    "\n",
    "There is a Python module called `torch-summary` that provides a more extensive model summary and displays the total number of parameters per each layer. You can check it out at this link: https://pypi.org/project/torch-summary/ \n",
    "\n",
    "However, I am not a fan of bloating my environments with unnecessary libraries, so I will show you how to manually look for your model's trainable parameters.\n",
    "\n",
    "This is done by calling the `model.parameters()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that this is a generator, meaning that we can get the parameters by making it an iterator and calling `next()` a bunch of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_iter = iter(model.parameters())\n",
    "some_params = next(param_iter)\n",
    "print(some_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_more_params = next(param_iter)\n",
    "print(some_more_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(some_params.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first extracted parameters have shape (5, 4), meaning that they are 20. But that's just one set of parameters (likely the weights of the first linear layer).\n",
    "\n",
    "In order to get the total number of parameters, we can just multiply the shape of each set of parameters and compute the overall sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_params = np.sum([np.prod(param.shape) for param in model.parameters()])\n",
    "print(\"Total number of parameters:\", n_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we don't really know what each set of parameters represents. Of course, we can guess that the first one that we extracted contained the weights of the first linear layer, while the second stored the bias terms.\n",
    "\n",
    "But if we want to be sure, we can use the `model.named_parameters()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {name: params.detach().numpy() for name, params in model.named_parameters()}\n",
    "with np.printoptions(precision=2, suppress=True):\n",
    "    print(param_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can used key, value pairs stored in `model.named_parameters()` to see how many parameters each layer has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, params in param_dict.items():\n",
    "    # print(name.split(\".\"))\n",
    "    temp, layer, weight_type = name.split(\".\")\n",
    "    layer = int(layer)\n",
    "    print(f\"Layer {layer} ({weight_type}): \\t{np.prod(params.shape):>2} parameters\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some final touches before training\n",
    "\n",
    "Okay, we have played enough with the parameters of our model. It's time to start some actual training. Almost.\n",
    "\n",
    "There are some last steps that we need to follow before beginning the training process.\n",
    "\n",
    "1) Prepare your training dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyIrisDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Check that the data produced by the dataloader has the right shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch, y_batch = next(iter(train_loader))\n",
    "print(\"x_batch shape:\", x_batch.shape)\n",
    "print(\"y_batch shape:\", y_batch.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Verify that the model's output looks as expected (it should be triplets of scores, since we have 3 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_batch = model(x_batch)\n",
    "print(\"Example of model's output (before training):\")\n",
    "print(pred_batch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Define the loss function (cross-entropy) and the optimizer (Adam will be fine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Make a function to evaluate your model's accuracy. In classification problems, accuracy is defined as\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Accuracy} = \\frac{\\text{correct predictions}}{\\text{total predictions}}=\\frac{1}{n} \\sum_{i=1}^{n} \\chi\\{\\hat{y}^{(i)} = y^{(i)} \\}\n",
    "\\end{equation}\n",
    "\n",
    "I normally use $\\hat{y}$ for the predicted labels, $y$ for the true labels, and $\\chi$ for the indicator function, but let's not get stuck on notation. The point is that accuracy is the average number of samples that are correctly predicted by our model.\n",
    "\n",
    "This can be computed by using `np.mean(predicted_labels == true_labels)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_accuracy(x, y):\n",
    "    x = torch.tensor(x, dtype=torch.float64) # cast to tensor\n",
    "    out_tensor = model(x) # model's output scores\n",
    "    out = out_tensor.detach().numpy()\n",
    "    pred = np.argmax(out, axis=1)\n",
    "    return np.mean(pred == y)\n",
    "\n",
    "print(\"Train accuracy before training:\", model_accuracy(x_train, y_train))\n",
    "print(\"Test accuracy before training:\", model_accuracy(x_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n",
    "\n",
    "Finally, time to train our model. We do that by iterating the `train_loader` for multiple \"epochs\".\n",
    "\n",
    "Here's a brief summary of the steps you need to follow for each batch:\n",
    "\n",
    "1) Reset the gradient stored in the optimizer with `optimizer.zero_grad()`. By default, your optimizer keeps the sums of all the gradients computed so far, so at each new iteration you normally want to clear it.\n",
    "\n",
    "2) Feed the batch to the model, and obtain the model's output scores (a.k.a. logits), which should be a tensor of size `(batch_size, n_classes)` \n",
    "\n",
    "3) Calculate the cross-entropy loss between the logits and the true labels (check the docs to see how the cross-entropy loss works https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
    "\n",
    "4) Call `loss_batch.backwards()` to perform the backpropagation, and compute the gradient, which is automatically added to the optimizer\n",
    "\n",
    "5) Use `optimizer.step()` to update the parameters based on the current gradient. In the standard SGD, this is simply\n",
    "\\begin{equation}\n",
    "w \\gets w - \\eta \\cdot \\nabla \\ell\n",
    "\\end{equation}\n",
    "where $\\nabla \\ell$ is the gradient of the loss, $w$ are the model parameters, and $\\eta$ is the learning rate. But in our case we are using Adam, so there are some additional adaptive momentum terms that are taken into account.\n",
    "\n",
    "Now, let the training begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "for _ in range(n_epochs):\n",
    "    for x_batch, y_batch in train_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scores_batch = model(x_batch)\n",
    "        loss_batch = loss_fn(scores_batch, y_batch)\n",
    "        loss_batch.backward()\n",
    "        optimizer.step()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went smoothly, your model should now have a decent accuracy.\n",
    "\n",
    "IMPORTANT: We are interested in the *test accuracy*. We compute the training accuracy only to see what is the gap between the two. That can help to determine if the model is <i>overfitting</i> the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train accuracy after training:\", model_accuracy(x_train, y_train))\n",
    "print(\"Test accuracy after training:\", model_accuracy(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {name: params.detach().numpy() for name, params in model.named_parameters()}\n",
    "with np.printoptions(precision=2, suppress=True):\n",
    "    print(param_dict[\"model.0.weight\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "Neural networks have various settings (or <i>hyperparameters</i>) that you can change. You may want to get acquainted with them a get an idea of what role they play in the training process.\n",
    "\n",
    "You can start by toying with the hyperparameters of this model and seeing what happens when you change:\n",
    "- Batch size\n",
    "- Learning rate\n",
    "- Architecture (try more layers, less layers, bigger layers, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hy673",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "898d28840f55b3c5c9a615fda231169adc20c90e3e87a937f55caa36837c15d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
